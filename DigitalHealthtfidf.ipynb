{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "h8MeUYkk8xyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "4q9jNdja81Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9JgUVqX79AL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "YpKPQrob9BqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUbJGlJA9ESG",
        "outputId": "bb8773f0-2926-4bfa-d1d5-02a8940d76da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anxiety = pd.read_csv('Copy of anxiety.csv')\n",
        "depression = pd.read_csv('Copy of depression_dataset.csv')"
      ],
      "metadata": {
        "id": "25JQbOVb8upS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anxiety = anxiety[anxiety['rawContent'].notna()]"
      ],
      "metadata": {
        "id": "ahvt1ndF9bWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing(text):\n",
        "    # Remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@)\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "\n",
        "    # Remove newline character\n",
        "    text = re.sub(r'\\n','', text)\n",
        "\n",
        "    # Everything except letters, numbers, and hashtags are replaced with a space.\n",
        "    text = re.sub(r\"[^A-Za-z0-9#]+\", ' ', text)\n",
        "\n",
        "    # Remove any extra spaces between words, and trailing or leading spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "depression['tweetCleaned'] = depression['rawContent'].apply(pre_processing)\n",
        "anxiety['tweetCleaned'] = anxiety['rawContent'].apply(pre_processing)"
      ],
      "metadata": {
        "id": "IrPz9KB69hsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Lowercasing\n",
        "text1 = depression['tweetCleaned']\n",
        "text2 = anxiety['tweetCleaned']\n",
        "\n",
        "d_tokens_list = [word_tokenize(i) for i in text1]\n",
        "a_tokens_list = [word_tokenize(i) for i in text2]\n",
        "\n",
        "d_lc_tokens_list = []\n",
        "a_lc_tokens_list = []\n",
        "\n",
        "for i in d_tokens_list:\n",
        "    d_lc_tokens_list.append([token.lower() for token in i])\n",
        "\n",
        "for i in a_tokens_list:\n",
        "    a_lc_tokens_list.append([token.lower() for token in i])\n"
      ],
      "metadata": {
        "id": "UloiWe3e9jkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Add the punctuation in the stop words set.\n",
        "stop_words.update(punctuation)\n",
        "stop_words.add(\"...\")\n",
        "stop_words.add(\"..\")\n",
        "stop_words.add(\"e.g\")\n",
        "depression_stop_words = stop_words\n",
        "anxiety_stop_words = stop_words\n",
        "depression_stop_words.add(\"depression\")\n",
        "anxiety_stop_words.add(\"anxiety\")\n",
        "\n",
        "depression_filtered_sentence = []\n",
        "for i in d_lc_tokens_list:\n",
        "    depression_filtered_sentence.append([token for token in i if token not in depression_stop_words])\n",
        "\n",
        "anxiety_filtered_sentence = []\n",
        "for i in a_lc_tokens_list:\n",
        "    anxiety_filtered_sentence.append([token for token in i if token not in anxiety_stop_words])\n",
        "\n",
        "# Remove Numbers\n",
        "depression_filtered_sentence = [ ' '.join(i) for i in depression_filtered_sentence ]\n",
        "depression_filtered_sentence = [ re.sub(r'\\d+', '', sentence) for sentence in depression_filtered_sentence ]\n",
        "\n",
        "anxiety_filtered_sentence = [ ' '.join(i) for i in anxiety_filtered_sentence ]\n",
        "anxiety_filtered_sentence = [ re.sub(r'\\d+', '', sentence) for sentence in anxiety_filtered_sentence ]\n"
      ],
      "metadata": {
        "id": "YIoZ6ZBJ9te2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming or Lemmatization\n",
        "\n",
        "stemming = False\n",
        "if stemming:\n",
        "  stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "  d_stemmed_tokens_list = []\n",
        "  a_stemmed_tokens_list = []\n",
        "\n",
        "  # Remove certain stemmed words - placeholder!!!\n",
        "  words_to_remove = []\n",
        "\n",
        "  for i in depression_filtered_sentence:\n",
        "    d_stemmed_tokens = [stemmer.stem(j) for j in i.split()]\n",
        "    d_stemmed_tokens = [token for token in d_stemmed_tokens if token not in words_to_remove]\n",
        "    d_stemmed_tokens_list.append(d_stemmed_tokens)\n",
        "\n",
        "  for i in anxiety_filtered_sentence:\n",
        "    a_stemmed_tokens = [stemmer.stem(j) for j in i.split()]\n",
        "    a_stemmed_tokens = [token for token in a_stemmed_tokens if token not in words_to_remove]\n",
        "    a_stemmed_tokens_list.append(a_stemmed_tokens)\n",
        "\n",
        "  # Number of tokens\n",
        "  duniques = np.unique([tok for doc in d_stemmed_tokens_list for tok in doc])\n",
        "  print(\"Number of tokens after stemming: {}\\n\".format(len(duniques)))\n",
        "\n",
        "  # Number of tokens\n",
        "  auniques = np.unique([tok for doc in a_stemmed_tokens_list for tok in doc])\n",
        "  print(\"Number of tokens after stemming: {}\\n\".format(len(auniques)))\n",
        "\n",
        "  print('After stemming depression:')\n",
        "  for i in d_stemmed_tokens_list[:10]:\n",
        "    for j in i:\n",
        "      print(j,end=\" \")\n",
        "    print(\" \")\n",
        "\n",
        "  print('After stemming anxiety:')\n",
        "  for i in a_stemmed_tokens_list[:10]:\n",
        "    for j in i:\n",
        "      print(j,end=\" \")\n",
        "    print(\" \")\n",
        "\n",
        "else:\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  d_lemmatized_tokens_list = []\n",
        "  a_lemmatized_tokens_list = []\n",
        "\n",
        "  for i in depression_filtered_sentence:\n",
        "      d_lemmatized_tokens_list.append([lemmatizer.lemmatize(j) for j in i.split()])\n",
        "\n",
        "  for i in anxiety_filtered_sentence:\n",
        "      a_lemmatized_tokens_list.append([lemmatizer.lemmatize(j) for j in i.split()])\n",
        "\n",
        "  # number of tokens\n",
        "  duniques = np.unique([tok for doc in d_lemmatized_tokens_list for tok in doc])\n",
        "  print(\"Number of tokens after lemmatization depression: {}\\n\".format(len(duniques)))\n",
        "\n",
        "  auniques = np.unique([tok for doc in a_lemmatized_tokens_list for tok in doc])\n",
        "  print(\"Number of tokens after lemmatization depression: {}\\n\".format(len(auniques)))\n",
        "\n",
        "  print('After lemmatization depression:')\n",
        "  for i in d_lemmatized_tokens_list[:10]:\n",
        "      for j in i:\n",
        "          print(j, end=\" \")\n",
        "      print(\" \")\n",
        "\n",
        "  print('After lemmatization anxiety:')\n",
        "  for i in a_lemmatized_tokens_list[:10]:\n",
        "      for j in i:\n",
        "          print(j, end=\" \")\n",
        "      print(\" \")\n",
        "\n",
        "  # Set stemmed to lemmatized tokens to continue working with the same list\n",
        "  d_stemmed_tokens_list = d_lemmatized_tokens_list\n",
        "  a_stemmed_tokens_list = a_lemmatized_tokens_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98mBNFsa9_yf",
        "outputId": "224cb538-f57b-4e5d-ba40-55f0e841f6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens after lemmatization depression: 24629\n",
            "\n",
            "Number of tokens after lemmatization depression: 36075\n",
            "\n",
            "After lemmatization depression:\n",
            "forced making decision move residential care bullying stopped age diagnosed developed autistic trates  \n",
            "watching season dexter  \n",
            "pretty sure maybe lazy idk  \n",
            "strive like half friend happy like seeing happy half trying top help  \n",
            "adhd benrey adhd w psychotic feature w psychotic feature r said  \n",
            "pretty sure past day feeling pretty good feel like actually better hard explain like feeling happier expressive always background  \n",
            "bad friend better understanding know need take manic episode amp move quickly try fix  \n",
            "mood stable c ee  \n",
            "dad told attitude ruined easter dinner vibe sorry randy  \n",
            "said understand father deciding amp getting used fact time go husband called hour saw grandfather father passed point amp get treatment  \n",
            "After lemmatization anxiety:\n",
            "would go  \n",
            "know pain ptsd due medical problem life  \n",
            "might autism get help instead ignored  \n",
            "like dark soul game male cry  \n",
            "wonderful husband accidentally dropped plate sink washing dish early morning amp woke mini heart attack gone work dontleaveme  \n",
            "yeah happened get car shaking uncontrollably panic attack repeatedly kept asking drug like bai  \n",
            "nambro incredibly important case many medication meant help adhd symptom known worsen symptom diagnosed younger age many doctor shy away prescribing appropriate medication adhd  \n",
            "excellent also paxil definitely stopped rumination  \n",
            "btw justine tell get soooo many multiple view starch vlog listen like xanax audible form  \n",
            "god forgive hate working dementia unit really rather take write work people idc shit really trigger  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Tfidf values\n",
        "\n",
        "d_list_tfidf = [\" \".join(stemmed_list) for stemmed_list in d_stemmed_tokens_list]\n",
        "vectorizer = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n",
        "X = vectorizer.fit_transform(d_list_tfidf)\n",
        "\n",
        "# Print the top tfidf values for the first document\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_index = X[0, :].nonzero()[1]\n",
        "tfidf_scores = zip(feature_index, [X[0, x] for x in feature_index])\n",
        "\n",
        "sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "i = 0\n",
        "for w, s in [(feature_names[i], score) for i, score in sorted_tfidf_scores]:\n",
        "    print(w, s)\n",
        "    i += 1\n",
        "    if i == 10:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAMPDAWq_X3L",
        "outputId": "7544b255-fd87-4297-b329-cd21cc85d0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "autistic 0.4357963955724096\n",
            "stopped 0.43381869492313113\n",
            "move 0.4264762121313035\n",
            "age 0.3935155037947275\n",
            "making 0.3525004858776144\n",
            "care 0.31837688103805917\n",
            "diagnosed 0.24397950605968452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unigrams and bigrams per document\n",
        "unigrams = []\n",
        "bigrams = []\n",
        "\n",
        "for stemmed_tokens in d_stemmed_tokens_list:\n",
        "  unigrams.append(stemmed_tokens)\n",
        "  bigrams.append(list(ngrams(stemmed_tokens, 2)))"
      ],
      "metadata": {
        "id": "xLIWHTeHAWsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Tfidf values\n",
        "\n",
        "a_list_tfidf = [\" \".join(stemmed_list) for stemmed_list in a_stemmed_tokens_list]\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(a_list_tfidf)\n",
        "\n",
        "\n",
        "# Print the top tfidf values for the first document\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_index = X[0, :].nonzero()[1]\n",
        "tfidf_scores = zip(feature_index, [X[0, x] for x in feature_index])\n",
        "\n",
        "sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "i = 0\n",
        "for w, s in [(feature_names[i], score) for i, score in sorted_tfidf_scores]:\n",
        "    print(w, s)\n",
        "    i += 1\n",
        "    if i == 10:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXDfRoP0CP-I",
        "outputId": "b76f75c4-6c57-4754-940a-b80aa37b78b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would 0.7430273991075669\n",
            "go 0.6692609985464898\n"
          ]
        }
      ]
    }
  ]
}